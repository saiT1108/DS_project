{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severity of storms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our project will focus on analyzing and predicting the severity of tornadoes across various regions in America in terms of property damage. To analyze this, we will take the last 10 years of tornado data to represent an approximate measure of it. To calculate this, we will establish a threshold for measuring the severity using the amount of property damage caused by tornadoes. We will predict the property damage of tornadoes and use a scale of low, medium, and high to represent the severity.  \n",
    "In addition, we will also compare attitudes across each of the 4 seasons - summer, spring, winter, fall, and see if there are any trends present across the 10 year span that shows when tornadoes are most common and damaging. The trends will help us be more accurate in predicting the property damage caused by the tornadoes per region during different seasons/times of the year. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Business understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our dataset details instances of severe weather across a 10 year period. The dataset shows us the date/year, the state the tornado occured in, deaths/injuries, and the property damage. We will be using this dataset to predict the property damage caused by future tornadoes in region acros the US. Bonus: We will predict deaths/injuries if we have more time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Proposed analytics solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " How we get to the target variable -- severity index\n",
    "The severity index will be calculated byâ€¦ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Read all data into one single dataframe\n",
    "df_all_data = pd.read_csv('./dataset/storm_event_details_2010.csv')\n",
    "\n",
    "for i in range(2011,2021):\n",
    "    df_temp = pd.read_csv(f'./dataset/storm_event_details_{i}.csv')\n",
    "    df_all_data = df_all_data.append(df_temp, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3721/3240677609.py:12: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df_hur = df_hur.assign(HARM_TOTAL=df_hur[cols].sum(1)).drop(cols,1)\n"
     ]
    }
   ],
   "source": [
    "## Remove unused columns and format continuous columns\n",
    "\n",
    "df_hur = df_all_data[df_all_data['EVENT_TYPE']=='Tornado']\n",
    "df_hur = df_hur.drop(columns=['TOR_OTHER_WFO', 'END_YEARMONTH', 'EVENT_TYPE', 'END_DATE_TIME', 'BEGIN_YEARMONTH', 'BEGIN_DAY', \n",
    "                                            'END_DAY', 'EPISODE_ID', 'EVENT_ID',\n",
    "                                           'TOR_OTHER_CZ_STATE','TOR_OTHER_CZ_FIPS','TOR_OTHER_CZ_NAME','DATA_SOURCE','EPISODE_NARRATIVE',\n",
    "                                            'EVENT_NARRATIVE','WFO','SOURCE','CZ_TIMEZONE','BEGIN_AZIMUTH','END_AZIMUTH','BEGIN_LAT',\n",
    "                                            'END_LAT','BEGIN_LON','END_LON','STATE_FIPS','BEGIN_RANGE','END_RANGE','DAMAGE_CROPS',\n",
    "                                            'BEGIN_TIME','END_TIME','BEGIN_LOCATION','END_LOCATION','FLOOD_CAUSE','MAGNITUDE_TYPE',\n",
    "                                            'MAGNITUDE','CZ_FIPS','CZ_TYPE','CZ_NAME','CATEGORY'])\n",
    "cols = ['INJURIES_INDIRECT', 'INJURIES_DIRECT', 'DEATHS_INDIRECT', 'DEATHS_DIRECT']\n",
    "df_hur = df_hur.assign(HARM_TOTAL=df_hur[cols].sum(1)).drop(cols,1)\n",
    "df_hur['TOR_AREA'] = df_hur['TOR_LENGTH']*df_hur['TOR_WIDTH']\n",
    "df_hur = df_hur.drop(columns=['TOR_LENGTH', 'TOR_WIDTH'])\n",
    "df_hur = df_hur.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       DAMAGE_PROPERTY\n",
      "75               5.00K\n",
      "304              1.50M\n",
      "617             10.00K\n",
      "731            750.00K\n",
      "732             10.00K\n",
      "...                ...\n",
      "688471           0.00K\n",
      "688472         250.00K\n",
      "688495          60.00K\n",
      "688497           0.00K\n",
      "688560           0.00K\n",
      "\n",
      "[12945 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "## Format DAMAGE_PROPERTY column to be float instead of object\n",
    "\n",
    "dmg = pd.DataFrame(df_hur['DAMAGE_PROPERTY'])\n",
    "\n",
    "print(dmg)\n",
    "for index, row in dmg.iterrows():\n",
    "    val = row['DAMAGE_PROPERTY']\n",
    "    if val[-1:] == 'B':\n",
    "        row['DAMAGE_PROPERTY'] = float(val[:-1])*1000000000\n",
    "    elif val[-1:] == 'M':\n",
    "        row['DAMAGE_PROPERTY'] = float(val[:-1])*1000000\n",
    "    elif val[-1:] == 'K':\n",
    "        row['DAMAGE_PROPERTY'] = float(val[:-1])*1000\n",
    "\n",
    "df_hur['DAMAGE_PROPERTY'] = dmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12945 entries, 100619 to 688560\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   STATE            12945 non-null  object \n",
      " 1   YEAR             12945 non-null  int64  \n",
      " 2   MONTH_NAME       12945 non-null  object \n",
      " 3   BEGIN_DATE_TIME  12945 non-null  object \n",
      " 4   DAMAGE_PROPERTY  12945 non-null  object \n",
      " 5   TOR_F_SCALE      12945 non-null  object \n",
      " 6   HARM_TOTAL       12945 non-null  int64  \n",
      " 7   TOR_AREA         12945 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 910.2+ KB\n",
      "EF0    51.117296\n",
      "EF1    35.737575\n",
      "EF2     9.852883\n",
      "EF3     2.576541\n",
      "EF4     0.628231\n",
      "EF5     0.087475\n",
      "Name: TOR_F_SCALE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Sort by priority variable and find data split percentages\n",
    "\n",
    "df_hur = df_hur.sort_values('DAMAGE_PROPERTY', ascending=False)\n",
    "df_hur.info()\n",
    "df_hur = df_hur.loc[df_hur['TOR_F_SCALE']!='EFU']\n",
    "print(df_hur['TOR_F_SCALE'].value_counts()/len(df_hur) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "April        21.677932\n",
       "May          20.548708\n",
       "June         12.413519\n",
       "July          7.093439\n",
       "March         6.727634\n",
       "August        6.067594\n",
       "October       5.137177\n",
       "November      4.580517\n",
       "January       4.413519\n",
       "February      4.159046\n",
       "September     3.618290\n",
       "December      3.562624\n",
       "Name: MONTH_NAME, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hur['MONTH_NAME'].value_counts()/len(df_hur) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH_NAME</th>\n",
       "      <th>BEGIN_DATE_TIME</th>\n",
       "      <th>DAMAGE_PROPERTY</th>\n",
       "      <th>TOR_F_SCALE</th>\n",
       "      <th>HARM_TOTAL</th>\n",
       "      <th>TOR_AREA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>514700</th>\n",
       "      <td>LOUISIANA</td>\n",
       "      <td>2018</td>\n",
       "      <td>April</td>\n",
       "      <td>13-APR-18 23:42:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EF0</td>\n",
       "      <td>0</td>\n",
       "      <td>927.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460312</th>\n",
       "      <td>KANSAS</td>\n",
       "      <td>2017</td>\n",
       "      <td>May</td>\n",
       "      <td>18-MAY-17 15:30:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EF0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19643</th>\n",
       "      <td>KANSAS</td>\n",
       "      <td>2010</td>\n",
       "      <td>September</td>\n",
       "      <td>14-SEP-10 21:55:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EF0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138630</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>2011</td>\n",
       "      <td>May</td>\n",
       "      <td>25-MAY-11 23:17:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EF0</td>\n",
       "      <td>0</td>\n",
       "      <td>369.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370476</th>\n",
       "      <td>SOUTH DAKOTA</td>\n",
       "      <td>2015</td>\n",
       "      <td>May</td>\n",
       "      <td>10-MAY-15 09:21:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EF0</td>\n",
       "      <td>0</td>\n",
       "      <td>1798.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411485</th>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>2016</td>\n",
       "      <td>May</td>\n",
       "      <td>09-MAY-16 15:06:00</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>EF4</td>\n",
       "      <td>1</td>\n",
       "      <td>3560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105547</th>\n",
       "      <td>MISSISSIPPI</td>\n",
       "      <td>2011</td>\n",
       "      <td>April</td>\n",
       "      <td>27-APR-11 17:26:00</td>\n",
       "      <td>900000.0</td>\n",
       "      <td>EF4</td>\n",
       "      <td>18</td>\n",
       "      <td>30198.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296685</th>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>2014</td>\n",
       "      <td>June</td>\n",
       "      <td>16-JUN-14 15:00:00</td>\n",
       "      <td>12000000.0</td>\n",
       "      <td>EF4</td>\n",
       "      <td>21</td>\n",
       "      <td>4025.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59112</th>\n",
       "      <td>MINNESOTA</td>\n",
       "      <td>2010</td>\n",
       "      <td>June</td>\n",
       "      <td>17-JUN-10 17:33:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EF4</td>\n",
       "      <td>15</td>\n",
       "      <td>29761.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126672</th>\n",
       "      <td>MISSISSIPPI</td>\n",
       "      <td>2011</td>\n",
       "      <td>April</td>\n",
       "      <td>27-APR-11 14:42:00</td>\n",
       "      <td>2500000.0</td>\n",
       "      <td>EF5</td>\n",
       "      <td>53</td>\n",
       "      <td>7986.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1257 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               STATE  YEAR MONTH_NAME     BEGIN_DATE_TIME DAMAGE_PROPERTY  \\\n",
       "514700     LOUISIANA  2018      April  13-APR-18 23:42:00             0.0   \n",
       "460312        KANSAS  2017        May  18-MAY-17 15:30:00             0.0   \n",
       "19643         KANSAS  2010  September  14-SEP-10 21:55:00             0.0   \n",
       "138630       ALABAMA  2011        May  25-MAY-11 23:17:00             0.0   \n",
       "370476  SOUTH DAKOTA  2015        May  10-MAY-15 09:21:00             0.0   \n",
       "...              ...   ...        ...                 ...             ...   \n",
       "411485      OKLAHOMA  2016        May  09-MAY-16 15:06:00       1000000.0   \n",
       "105547   MISSISSIPPI  2011      April  27-APR-11 17:26:00        900000.0   \n",
       "296685      NEBRASKA  2014       June  16-JUN-14 15:00:00      12000000.0   \n",
       "59112      MINNESOTA  2010       June  17-JUN-10 17:33:00             0.0   \n",
       "126672   MISSISSIPPI  2011      April  27-APR-11 14:42:00       2500000.0   \n",
       "\n",
       "       TOR_F_SCALE  HARM_TOTAL  TOR_AREA  \n",
       "514700         EF0           0     927.0  \n",
       "460312         EF0           0       5.5  \n",
       "19643          EF0           0       6.5  \n",
       "138630         EF0           0     369.0  \n",
       "370476         EF0           0    1798.0  \n",
       "...            ...         ...       ...  \n",
       "411485         EF4           1    3560.0  \n",
       "105547         EF4          18   30198.0  \n",
       "296685         EF4          21    4025.0  \n",
       "59112          EF4          15   29761.6  \n",
       "126672         EF5          53    7986.0  \n",
       "\n",
       "[1257 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Split data into strata and sample proportinally (stratified sampling)\n",
    "\n",
    "sampled_df = df_hur.groupby('TOR_F_SCALE', group_keys=False).apply(lambda x: x.sample(frac=0.1))\n",
    "sampled_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
