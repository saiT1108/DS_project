{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severity of storms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our project will focus on analyzing and predicting the severity of tornadoes across various regions in America in terms of property damage. To analyze this, we will take the last 10 years of tornado data to represent an approximate measure of it. To calculate this, we will establish a threshold for measuring the severity using the amount of property damage caused by tornadoes. We will predict the property damage of tornadoes and use a scale of low, medium, and high to represent the severity.  \n",
    "In addition, we will also compare attitudes across each of the 4 seasons - summer, spring, winter, fall, and see if there are any trends present across the 10 year span that shows when tornadoes are most common and damaging. The trends will help us be more accurate in predicting the property damage caused by the tornadoes per region during different seasons/times of the year. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Business understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our dataset details instances of severe weather across a 10 year period. The dataset shows us the date/year, the state the tornado occured in, deaths/injuries, and the property damage. We will be using this dataset to predict the property damage caused by future tornadoes in region acros the US. 
     bonus: We will predict deaths/injuries if we have more time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Proposed analytics solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " How we get to the target variable -- severity index\n",
    "The severity index will be calculated by… \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Read all data into one single dataframe\n",
    "df_all_data = pd.read_csv('./dataset/storm_event_details_2010.csv')\n",
    "\n",
    "for i in range(2011,2021):\n",
    "    df_temp = pd.read_csv(f'./dataset/storm_event_details_{i}.csv')\n",
    "    df_all_data = df_all_data.append(df_temp, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10173/2115226861.py:11: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df_hur = df_hur.assign(HARM_TOTAL=df_hur[cols].sum(1)).drop(cols,1)\n"
     ]
    }
   ],
   "source": [
    "## Remove unused columns and format continuous columns\n",
    "\n",
    "df_hur = df_all_data[df_all_data['EVENT_TYPE']=='Tornado']\n",
    "df_hur = df_hur.drop(columns=['TOR_OTHER_WFO', 'END_YEARMONTH', 'EVENT_TYPE', 'END_DATE_TIME',\n",
    "                                           'TOR_OTHER_CZ_STATE','TOR_OTHER_CZ_FIPS','TOR_OTHER_CZ_NAME','DATA_SOURCE','EPISODE_NARRATIVE',\n",
    "                                            'EVENT_NARRATIVE','WFO','SOURCE','CZ_TIMEZONE','BEGIN_AZIMUTH','END_AZIMUTH','BEGIN_LAT',\n",
    "                                            'END_LAT','BEGIN_LON','END_LON','STATE_FIPS','BEGIN_RANGE','END_RANGE','DAMAGE_CROPS',\n",
    "                                            'BEGIN_TIME','END_TIME','BEGIN_LOCATION','END_LOCATION','FLOOD_CAUSE','MAGNITUDE_TYPE',\n",
    "                                            'MAGNITUDE','CZ_FIPS','CZ_TYPE','CZ_NAME','CATEGORY'])\n",
    "cols = ['INJURIES_INDIRECT', 'INJURIES_DIRECT', 'DEATHS_INDIRECT', 'DEATHS_DIRECT']\n",
    "df_hur = df_hur.assign(HARM_TOTAL=df_hur[cols].sum(1)).drop(cols,1)\n",
    "df_hur['TOR_AREA'] = df_hur['TOR_LENGTH']*df_hur['TOR_WIDTH']\n",
    "df_hur = df_hur.drop(columns=['TOR_LENGTH', 'TOR_WIDTH'])\n",
    "df_hur = df_hur.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       DAMAGE_PROPERTY\n",
      "75               5.00K\n",
      "304              1.50M\n",
      "617             10.00K\n",
      "731            750.00K\n",
      "732             10.00K\n",
      "...                ...\n",
      "688471           0.00K\n",
      "688472         250.00K\n",
      "688495          60.00K\n",
      "688497           0.00K\n",
      "688560           0.00K\n",
      "\n",
      "[12945 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "## Format DAMAGE_PROPERTY column to be float instead of object\n",
    "\n",
    "dmg = pd.DataFrame(df_hur['DAMAGE_PROPERTY'])\n",
    "\n",
    "print(dmg)\n",
    "for index, row in dmg.iterrows():\n",
    "    val = row['DAMAGE_PROPERTY']\n",
    "    if val[-1:] == 'B':\n",
    "        row['DAMAGE_PROPERTY'] = float(val[:-1])*1000000000\n",
    "    elif val[-1:] == 'M':\n",
    "        row['DAMAGE_PROPERTY'] = float(val[:-1])*1000000\n",
    "    elif val[-1:] == 'K':\n",
    "        row['DAMAGE_PROPERTY'] = float(val[:-1])*1000\n",
    "\n",
    "df_hur['DAMAGE_PROPERTY'] = dmg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12945 entries, 100619 to 688560\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   BEGIN_YEARMONTH  12945 non-null  int64  \n",
      " 1   BEGIN_DAY        12945 non-null  int64  \n",
      " 2   END_DAY          12945 non-null  int64  \n",
      " 3   EPISODE_ID       12945 non-null  int64  \n",
      " 4   EVENT_ID         12945 non-null  int64  \n",
      " 5   STATE            12945 non-null  object \n",
      " 6   YEAR             12945 non-null  int64  \n",
      " 7   MONTH_NAME       12945 non-null  object \n",
      " 8   BEGIN_DATE_TIME  12945 non-null  object \n",
      " 9   DAMAGE_PROPERTY  12945 non-null  object \n",
      " 10  TOR_F_SCALE      12945 non-null  object \n",
      " 11  HARM_TOTAL       12945 non-null  int64  \n",
      " 12  TOR_AREA         12945 non-null  float64\n",
      "dtypes: float64(1), int64(7), object(5)\n",
      "memory usage: 1.4+ MB\n",
      "EF0    51.117296\n",
      "EF1    35.737575\n",
      "EF2     9.852883\n",
      "EF3     2.576541\n",
      "EF4     0.628231\n",
      "EF5     0.087475\n",
      "Name: TOR_F_SCALE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Sort by priority variable and find data split percentages\n",
    "\n",
    "df_hur = df_hur.sort_values('DAMAGE_PROPERTY', ascending=False)\n",
    "df_hur.info()\n",
    "df_hur = df_hur.loc[df_hur['TOR_F_SCALE']!='EFU']\n",
    "print(df_hur['TOR_F_SCALE'].value_counts()/len(df_hur) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "April        21.677932\n",
       "May          20.548708\n",
       "June         12.413519\n",
       "July          7.093439\n",
       "March         6.727634\n",
       "August        6.067594\n",
       "October       5.137177\n",
       "November      4.580517\n",
       "January       4.413519\n",
       "February      4.159046\n",
       "September     3.618290\n",
       "December      3.562624\n",
       "Name: MONTH_NAME, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hur['MONTH_NAME'].value_counts()/len(df_hur) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH_NAME</th>\n",
       "      <th>BEGIN_DATE_TIME</th>\n",
       "      <th>DAMAGE_PROPERTY</th>\n",
       "      <th>TOR_F_SCALE</th>\n",
       "      <th>HARM_TOTAL</th>\n",
       "      <th>TOR_AREA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97183</th>\n",
       "      <td>201105</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>51059</td>\n",
       "      <td>302355</td>\n",
       "      <td>MISSOURI</td>\n",
       "      <td>2011</td>\n",
       "      <td>May</td>\n",
       "      <td>25-MAY-11 15:44:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>EF0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329697</th>\n",
       "      <td>201506</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>97273</td>\n",
       "      <td>584944</td>\n",
       "      <td>OHIO</td>\n",
       "      <td>2015</td>\n",
       "      <td>June</td>\n",
       "      <td>23-JUN-15 12:39:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>EF0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311332</th>\n",
       "      <td>201405</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>85812</td>\n",
       "      <td>518123</td>\n",
       "      <td>KANSAS</td>\n",
       "      <td>2014</td>\n",
       "      <td>May</td>\n",
       "      <td>11-MAY-14 20:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EF0</td>\n",
       "      <td>0</td>\n",
       "      <td>93.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197452</th>\n",
       "      <td>201212</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>69591</td>\n",
       "      <td>417582</td>\n",
       "      <td>FLORIDA</td>\n",
       "      <td>2012</td>\n",
       "      <td>December</td>\n",
       "      <td>20-DEC-12 17:42:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EF0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220953</th>\n",
       "      <td>201308</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>76280</td>\n",
       "      <td>459929</td>\n",
       "      <td>COLORADO</td>\n",
       "      <td>2013</td>\n",
       "      <td>August</td>\n",
       "      <td>03-AUG-13 14:37:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EF0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77158</th>\n",
       "      <td>201104</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>49915</td>\n",
       "      <td>296789</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>2011</td>\n",
       "      <td>April</td>\n",
       "      <td>27-APR-11 19:27:00</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>EF4</td>\n",
       "      <td>108</td>\n",
       "      <td>3920.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296920</th>\n",
       "      <td>201406</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>84971</td>\n",
       "      <td>514042</td>\n",
       "      <td>NEBRASKA</td>\n",
       "      <td>2014</td>\n",
       "      <td>June</td>\n",
       "      <td>16-JUN-14 15:17:00</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>EF4</td>\n",
       "      <td>1</td>\n",
       "      <td>4300.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103448</th>\n",
       "      <td>201104</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>49915</td>\n",
       "      <td>300460</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>2011</td>\n",
       "      <td>April</td>\n",
       "      <td>27-APR-11 19:13:00</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>EF4</td>\n",
       "      <td>14</td>\n",
       "      <td>14344.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244989</th>\n",
       "      <td>201302</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>71831</td>\n",
       "      <td>433413</td>\n",
       "      <td>MISSISSIPPI</td>\n",
       "      <td>2013</td>\n",
       "      <td>February</td>\n",
       "      <td>10-FEB-13 17:03:00</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>EF4</td>\n",
       "      <td>8</td>\n",
       "      <td>9966.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141427</th>\n",
       "      <td>201104</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>51541</td>\n",
       "      <td>309261</td>\n",
       "      <td>MISSISSIPPI</td>\n",
       "      <td>2011</td>\n",
       "      <td>April</td>\n",
       "      <td>27-APR-11 13:43:00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>EF5</td>\n",
       "      <td>9</td>\n",
       "      <td>2601.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1257 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BEGIN_YEARMONTH  BEGIN_DAY  END_DAY  EPISODE_ID  EVENT_ID  \\\n",
       "97183            201105         25       25       51059    302355   \n",
       "329697           201506         23       23       97273    584944   \n",
       "311332           201405         11       11       85812    518123   \n",
       "197452           201212         20       20       69591    417582   \n",
       "220953           201308          3        3       76280    459929   \n",
       "...                 ...        ...      ...         ...       ...   \n",
       "77158            201104         27       27       49915    296789   \n",
       "296920           201406         16       16       84971    514042   \n",
       "103448           201104         27       27       49915    300460   \n",
       "244989           201302         10       10       71831    433413   \n",
       "141427           201104         27       27       51541    309261   \n",
       "\n",
       "              STATE  YEAR MONTH_NAME     BEGIN_DATE_TIME DAMAGE_PROPERTY  \\\n",
       "97183      MISSOURI  2011        May  25-MAY-11 15:44:00             2.0   \n",
       "329697         OHIO  2015       June  23-JUN-15 12:39:00             3.0   \n",
       "311332       KANSAS  2014        May  11-MAY-14 20:51:00             0.0   \n",
       "197452      FLORIDA  2012   December  20-DEC-12 17:42:00             0.0   \n",
       "220953     COLORADO  2013     August  03-AUG-13 14:37:00             0.0   \n",
       "...             ...   ...        ...                 ...             ...   \n",
       "77158     TENNESSEE  2011      April  27-APR-11 19:27:00         20000.0   \n",
       "296920     NEBRASKA  2014       June  16-JUN-14 15:17:00          1000.0   \n",
       "103448    TENNESSEE  2011      April  27-APR-11 19:13:00         15000.0   \n",
       "244989  MISSISSIPPI  2013   February  10-FEB-13 17:03:00         13500.0   \n",
       "141427  MISSISSIPPI  2011      April  27-APR-11 13:43:00           100.0   \n",
       "\n",
       "       TOR_F_SCALE  HARM_TOTAL  TOR_AREA  \n",
       "97183          EF0           0     24.50  \n",
       "329697         EF0           0     11.75  \n",
       "311332         EF0           0     93.75  \n",
       "197452         EF0           0      0.10  \n",
       "220953         EF0           0      5.00  \n",
       "...            ...         ...       ...  \n",
       "77158          EF4         108   3920.00  \n",
       "296920         EF4           1   4300.00  \n",
       "103448         EF4          14  14344.00  \n",
       "244989         EF4           8   9966.00  \n",
       "141427         EF5           9   2601.00  \n",
       "\n",
       "[1257 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Split data into strata and sample proportinally (stratified sampling)\n",
    "\n",
    "sampled_df = df_hur.groupby('TOR_F_SCALE', group_keys=False).apply(lambda x: x.sample(frac=0.1))\n",
    "sampled_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
